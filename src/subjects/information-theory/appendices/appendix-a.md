---
title: Appendix A - Notation
layout: subject-layout.njk
---

# Appendix A - Notation

This appendix provides a quick reference for the common mathematical notation used throughout these notes on Information Theory.

-   `X, Y, Z`: Random variables.
-   `x, y, z`: Specific outcomes or values of random variables.
-   `P(X=x)` or `p(x)`: Probability of random variable `X` taking value `x`.
-   `p(x, y)`: Joint probability of `X=x` and `Y=y`.
-   `p(x | y)`: Conditional probability of `X=x` given `Y=y`.
-   `log₂`: Logarithm to the base 2 (used for bits).
-   `H(X)`: Shannon Entropy of random variable `X`.
-   `H(X | Y)`: Conditional Entropy of `X` given `Y`.
-   `H(X, Y)`: Joint Entropy of `X` and `Y`.
-   `I(X; Y)`: Mutual Information between `X` and `Y`.
-   `D_KL(P || Q)`: Kullback-Leibler Divergence (Relative Entropy) from distribution `Q` to `P`.n-   `E[...]`: Expectation operator.
-   `Σ`: Summation.
-   `∫`: Integration.
-   `C`: Channel Capacity.
-   `R`: Transmission Rate.
-   `P_e`: Probability of error.
